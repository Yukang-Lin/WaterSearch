import os
import json
import argparse
import numpy as np
import openai
from datasets import load_dataset
from alpaca_farm.auto_annotations import alpaca_leaderboard
import datasets
import re

from metrics import (
    qa_f1_score,
    rouge_zh_score,
    qa_f1_zh_score,
    rouge_score,
    classification_score,
    retrieval_score,
    retrieval_zh_score,
    count_score,
    code_sim_score,
)
import os

openai.api_key = "your_api_key"
openai.api_base = "base_url"

dataset2metric = {
    "narrativeqa": qa_f1_score,
    "qasper": qa_f1_score,
    "multifieldqa_en": qa_f1_score,
    "multifieldqa_zh": qa_f1_zh_score,
    "hotpotqa": qa_f1_score,
    "2wikimqa": qa_f1_score,
    "musique": qa_f1_score,
    "dureader": rouge_zh_score,
    "gov_report": rouge_score,
    "qmsum": rouge_score,
    "multi_news": rouge_score,
    "vcsum": rouge_zh_score,
    "trec": classification_score,
    "triviaqa": qa_f1_score,
    "samsum": rouge_score,
    "lsht": classification_score,
    "passage_retrieval_en": retrieval_score,
    "passage_count": count_score,
    "passage_retrieval_zh": retrieval_zh_score,
    "lcc": code_sim_score,
    "repobench-p": code_sim_score,
    "knowledge_memorization": qa_f1_score,
    "knowledge_understanding": qa_f1_score,
    "longform_qa": rouge_score,
    "finance_qa": rouge_score,
}

def parse_args(args=None):
    parser = argparse.ArgumentParser(description="Evaluate texts generated by every method")

    parser.add_argument(
        "--input_dir",
        type=str,
        default="/data2/tsq/WaterBench/pred/llama2-7b-chat-4k_no_g0.5_d5.0")
    args = parser.parse_args()

    return args
def clean_repetitions(text: str, 
                     word_ngram_size: int = 3,
                     char_ngram_size: int = 6,
                     min_word_repeat: int = 2,
                     min_char_repeat: int = 3,
                     enable_char_level: bool = True,
                     enable_regex_patterns: bool = True,
                     preserve_spaces: bool = True) -> str:
    if not text or not text.strip():
        return text
    
    if enable_regex_patterns:
        text = _clean_regex_patterns(text)
    
    text = _clean_word_repetitions(text, word_ngram_size, min_word_repeat)
    
    if enable_char_level:
        text = _clean_char_repetitions(text, char_ngram_size, min_char_repeat, preserve_spaces)
    
    text = _normalize_spaces(text)
    
    return text

def _clean_regex_patterns(text: str) -> str:
    
    text = re.sub(r'(.)\1{4,}', r'\1\1', text)
    
    for pattern_len in range(2, 11):
        pattern = r'(.{' + str(pattern_len) + r'})\1{2,}'
        text = re.sub(pattern, r'\1', text)
    
    text = re.sub(r'\b(\w+)(\s+\1\b){2,}', r'\1', text)
    
    return text

def _clean_word_repetitions(text: str, ngram_size: int, min_repeat: int) -> str:
    words = text.split()
    if len(words) < ngram_size:
        return text
    
    ngram_counts = {}
    result = []
    
    for i in range(len(words)):
        if i < ngram_size - 1:
            result.append(words[i])
        else:
            current_ngram = tuple(words[i-ngram_size+1:i+1])
            ngram_counts[current_ngram] = ngram_counts.get(current_ngram, 0) + 1
            
            if ngram_counts[current_ngram] >= min_repeat:
                break
            else:
                result.append(words[i])
    
    return ' '.join(result)

def _clean_char_repetitions(text: str, ngram_size: int, min_repeat: int, preserve_spaces: bool) -> str:
    if len(text) < ngram_size:
        return text
    
    if preserve_spaces:
        parts = text.split(' ')
        cleaned_parts = []
        
        for part in parts:
            if part:
                cleaned_part = _clean_char_repetitions_core(part, ngram_size, min_repeat)
                cleaned_parts.append(cleaned_part)
            else:
                cleaned_parts.append(part)
        
        return ' '.join(cleaned_parts)
    else:
        return _clean_char_repetitions_core(text, ngram_size, min_repeat)

def _clean_char_repetitions_core(text: str, ngram_size: int, min_repeat: int) -> str:
    if len(text) < ngram_size:
        return text
    
    ngram_counts = {}
    result = []
    
    for i in range(len(text)):
        if i < ngram_size - 1:
            result.append(text[i])
        else:
            current_ngram = text[i-ngram_size+1:i+1]
            ngram_counts[current_ngram] = ngram_counts.get(current_ngram, 0) + 1
            
            if ngram_counts[current_ngram] >= min_repeat:
                break
            else:
                result.append(text[i])
    
    return ''.join(result)

def _normalize_spaces(text: str) -> str:
    text = re.sub(r'\s+', ' ', text)
    return text.strip()

def clean_repetitions_strict(text: str) -> str:
    return clean_repetitions(
        text,
        word_ngram_size=2,
        char_ngram_size=4,
        min_word_repeat=2,
        min_char_repeat=2,
        enable_char_level=True,
        enable_regex_patterns=True
    )

score_list = []
def scorer(dataset, predictions, answers, all_classes):
    total_score = 0.

    for (prediction, ground_truths) in zip(predictions, answers):
        score = 0.
        if dataset in ["trec", "triviaqa", "samsum", "lsht"]:
            prediction = prediction.lstrip('\n').split('\n')[0]
        # prediction = clean_repetitions(prediction)
        for ground_truth in ground_truths:
            score = max(score, dataset2metric[dataset](prediction, ground_truth, all_classes=all_classes))
            score_list.append(score)
        total_score += score
    return round(100 * total_score / len(predictions), 2)

def alpacafarm_score(prompts, predictions, model_name):
    # outputs should be a list of json as such:
    # [{'instruction': 'What are the names of some famous actors that started their careers on Broadway?', 'input': '', 'output': 'Some famous actors that started their careers on Broadway are Hugh Jackman, Meryl Streep, Denzel Washington, Audra McDonald, and Lin-Manuel Miranda.', 'generator': 'gpt-3.5-turbo-0301', 'dataset': 'helpful_base', 'datasplit': 'eval'},
    # ...]
    my_outputs = []
    
    alpaca_eval_data = load_dataset("tatsu-lab/alpaca_farm", "alpaca_farm_evaluation")["eval"]
    for i, line in enumerate(alpaca_eval_data):
        # json_obj = json.loads(line)
        prompt = line["instruction"]
        _input = line["input"]
        if i >= len(predictions):
            continue
        prediction = predictions[i]
        my_outputs.append({"instruction": prompt, "input": _input, "generator": model_name, "output": prediction})
    print("my_outputs[0] is:", my_outputs[0])
    df_results = alpaca_leaderboard(
        path_or_all_outputs=my_outputs,
        name=model_name,
        is_add_reference_methods=False,
        annotators_config = "greedy_gpt4/configs.yaml"
    )
    print(df_results)
    score = round(df_results["win_rate"].iloc[0], 2)
    # score = df_results.to_string(float_format="%.2f")
    return score


if __name__ == '__main__':
    args = parse_args()
    scores = dict()
    # get all files from input_dir
    files = os.listdir(args.input_dir)
    model_name = args.input_dir.split("/")[-1]
    # get all json files
    json_files = [f for f in files if f.endswith(".jsonl")]
    save_dir =  os.path.join(args.input_dir, "eval")
    os.makedirs(save_dir, exist_ok=True)
    print("Evaluating on:", files)
    for json_file in json_files:
        if not json_file.endswith("jsonl"):
            continue
        print(f"{json_file} has began.........")
        # read jsons
        dataset = json_file.split(".")[0]
        predictions, answers, lengths, all_classes = [], [], [], []
        with open(os.path.join(args.input_dir, json_file), "r") as f:
            # lines
            lines = f.readlines()
            # texts
            prompts = [json.loads(line)["prompt"] for line in lines]
            predictions = [json.loads(line)["pred"] for line in lines]
            answers = [json.loads(line)["answers"] for line in lines]
            all_classes = json.loads(lines[0])["all_classes"]
            print(f"predictions[0] is: {predictions[0]}")
            if dataset == "alpacafarm":
                score = alpacafarm_score(prompts, predictions, model_name)
            else:
                score = scorer(dataset, predictions, answers, all_classes)
            scores[dataset] = score
    # save
    out_path = os.path.join(save_dir, "result.json")
    with open(out_path, "w") as f:
        json.dump(scores, f, ensure_ascii=False, indent=4)
